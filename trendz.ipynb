{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "from IPython.display import display\n",
    "\n",
    "# Load SpaCy model & Sentiment Analyzer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Get today's date in IST\n",
    "current_date_ist = datetime.now(pytz.timezone('Asia/Kolkata')).date()\n",
    "\n",
    "# Function to extract article content\n",
    "def fetch_article_content(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = ' '.join(p.text.strip() for p in paragraphs if p.text.strip())\n",
    "        return content if content else \"N/A\"\n",
    "    except Exception:\n",
    "        return \"Error fetching content\"\n",
    "\n",
    "# Function to extract Named Entities and Sentiment\n",
    "def process_text(text):\n",
    "    doc = nlp(text)\n",
    "    sentiment_score = sia.polarity_scores(text)[\"compound\"]\n",
    "    sentiment = \"Positive\" if sentiment_score > 0.2 else \"Negative\" if sentiment_score < -0.2 else \"Neutral\"\n",
    "\n",
    "    entities = {\"PERSON\": [], \"LOC\": [], \"ORG\": [], \"MISC\": []}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"PERSON\"]:\n",
    "            entities[\"PERSON\"].append(ent.text)\n",
    "        elif ent.label_ in [\"GPE\", \"LOC\"]:\n",
    "            entities[\"LOC\"].append(ent.text)\n",
    "        elif ent.label_ in [\"ORG\"]:\n",
    "            entities[\"ORG\"].append(ent.text)\n",
    "        else:\n",
    "            entities[\"MISC\"].append(ent.text)\n",
    "\n",
    "    # Extract Important Words\n",
    "    words = [token.text.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
    "    keyword_counts = Counter(words)\n",
    "    keywords = [word for word, freq in keyword_counts.most_common(10)]  # Top 10 Keywords\n",
    "\n",
    "    return sentiment, sentiment_score, entities, keywords\n",
    "\n",
    "# Function to fetch articles metadata and content\n",
    "def Fetch_Articles_Metadata(channel_names, channels_xml_links):\n",
    "    data = []\n",
    "    for i, links in enumerate(channels_xml_links):\n",
    "        for xml_url in links:\n",
    "            try:\n",
    "                response = requests.get(xml_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                response.encoding = 'utf-8'\n",
    "                root = ET.fromstring(response.text)\n",
    "                namespace = {\n",
    "                    'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9',\n",
    "                    'news': 'http://www.google.com/schemas/sitemap-news/0.9'\n",
    "                }\n",
    "                for url in root.findall('sitemap:url', namespace):\n",
    "                    title = url.find('news:news/news:title', namespace)\n",
    "                    title = title.text.strip() if title is not None else \"N/A\"\n",
    "                    link = url.find('sitemap:loc', namespace)\n",
    "                    link = link.text.strip() if link is not None else \"N/A\"\n",
    "                    published_datetime = url.find('news:news/news:publication_date', namespace)\n",
    "                    published_datetime = published_datetime.text.strip() if published_datetime is not None else None\n",
    "                    if published_datetime:\n",
    "                        try:\n",
    "                            parsed_datetime = datetime.strptime(published_datetime, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "                            parsed_datetime_ist = parsed_datetime.astimezone(pytz.timezone('Asia/Kolkata'))\n",
    "\n",
    "                            # **Check if the article was published today**\n",
    "                            if parsed_datetime_ist.date() == current_date_ist:\n",
    "                                content = fetch_article_content(link)\n",
    "                                sentiment, sentiment_score, entities, keywords = process_text(content)\n",
    "                                data.append({\n",
    "                                    'Source': channel_names[i],\n",
    "                                    'Title': title,\n",
    "                                    'Link': link,\n",
    "                                    'Published': parsed_datetime_ist.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                                    'Sentiment': sentiment,\n",
    "                                    'Sentiment Score': sentiment_score,\n",
    "                                    'Persons': ', '.join(set(entities[\"PERSON\"])),\n",
    "                                    'Locations': ', '.join(set(entities[\"LOC\"])),\n",
    "                                    'Organizations': ', '.join(set(entities[\"ORG\"])),\n",
    "                                    'Miscellaneous': ', '.join(set(entities[\"MISC\"])),\n",
    "                                    'Keywords': ', '.join(set(keywords))\n",
    "                                })\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "            except Exception:\n",
    "                continue\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Define News Sources\n",
    "channel_names = [\"The Hindu\", \"Hindustan Times\", \"NDTV\", \"News18\", \"Zee News\", \"Jagran\", \"Firstpost\", \"Indian Express\", \"LiveMint\", \"business standard\",\"India.com\", \"Indiatoday\", \"bhaskar\", \"dnaindia\"]\n",
    "channels_xml_links = [\n",
    "    [\"https://www.thehindu.com/sitemap/googlenews/all/all.xml\"],\n",
    "    [\"https://www.hindustantimes.com/sitemap/news.xml\"],\n",
    "    [\"https://www.ndtv.com/sitemap.xml\"],\n",
    "    [\"https://www.news18.com/commonfeeds/v1/eng/sitemap/google-news/today.xml\"],\n",
    "    [\"https://zeenews.india.com/sitemap.xml\"],\n",
    "    [\"https://english.jagran.com/news-sitemap.xml\"],\n",
    "    [\"https://www.firstpost.com/commonfeeds/v1/mfp/sitemap/google-news.xml\"],\n",
    "    [\"https://indianexpress.com/news-sitemap.xml\"],\n",
    "    [\"https://www.livemint.com/sitemap/today.xml\"],\n",
    "    ['https://www.business-standard.com/sitemap/news-sitemap.xml'],\n",
    "    ['https://www.india.com/google-news-sitemap.xml'],\n",
    "    ['https://www.indiatoday.in/sitemapindex.xml'],\n",
    "    ['https://www.bhaskarenglish.in/sitemaps-v1--sitemap-google-news-1.xml'],\n",
    "    ['https://www.dnaindia.com/googlenews.xml']\n",
    "]\n",
    "\n",
    "# Fetch Data\n",
    "df = Fetch_Articles_Metadata(channel_names, channels_xml_links)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "import plotly.express as px\n",
    "import geopandas as gpd\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if not df.empty:\n",
    "    ### 1. SENTIMENT ANALYSIS BAR CHART ###\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.countplot(x='Sentiment', data=df, palette={\"Positive\": \"green\", \"Neutral\": \"gray\", \"Negative\": \"red\"})\n",
    "    plt.title(\"1. Sentiment Analysis of News Articles\")\n",
    "    plt.xlabel(\"Sentiment\")\n",
    "    plt.ylabel(\"Number of Articles\")\n",
    "    plt.show()\n",
    "\n",
    "    ### 2. TOP KEYWORDS WORDCLOUD ###\n",
    "    wordcloud_text = \" \".join(df['Persons'].dropna())\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='black', colormap='coolwarm').generate(wordcloud_text)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"2. Top Keywords in News Articles\")\n",
    "    plt.show()\n",
    "\n",
    "    ### 3. TOP LOCATIONS MENTIONED IN NEWS ###\n",
    "    locations = df['Locations'].dropna().str.split(', ').explode()\n",
    "    top_locations = locations.value_counts().head(10)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=top_locations.index, y=top_locations.values, palette=\"magma\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(\"3. Top 10 Locations in News Articles\")\n",
    "    plt.xlabel(\"Location\")\n",
    "    plt.ylabel(\"Number of Mentions\")\n",
    "    plt.show()\n",
    "\n",
    "    ### 4. ORGANIZATIONS IN NEWS ###\n",
    "    organizations = df['Organizations'].dropna().str.split(', ').explode()\n",
    "    top_organizations = organizations.value_counts().head(10)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=top_organizations.index, y=top_organizations.values, palette=\"coolwarm\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(\"4. Top 10 Organizations in News Articles\")\n",
    "    plt.xlabel(\"Organization\")\n",
    "    plt.ylabel(\"Number of Mentions\")\n",
    "    plt.show()\n",
    "\n",
    "    ### 5. Source vs Sentiment ###\n",
    "    plt.subplots(figsize=(8, 8))\n",
    "    plt.title(\"5. Source vs Sentiment\")\n",
    "    df_2dhist = pd.DataFrame({\n",
    "        x_label: grp['Sentiment'].value_counts()\n",
    "        for x_label, grp in df.groupby('Source')\n",
    "    })\n",
    "    sns.heatmap(df_2dhist, cmap='viridis')\n",
    "    plt.xlabel('Source')\n",
    "    _ = plt.ylabel('Sentiment')\n",
    "\n",
    "    ### 6. Source vs Sentiment Score ###\n",
    "    figsize = (12, 1.2 * len(df['Source'].unique()))\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(\"6. Source vs Sentiment Score\")\n",
    "    sns.violinplot(df, x='Sentiment Score', y='Source', inner='stick', palette='Dark2')\n",
    "    sns.despine(top=True, right=True, bottom=True, left=True)\n",
    "\n",
    "    ### 7. Sentiment vs Sentiment Score ###\n",
    "    figsize = (12, 1.2 * len(df['Sentiment'].unique()))\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(\"7. Sentiment vs Sentiment Score\")\n",
    "    sns.violinplot(df, x='Sentiment Score', y='Sentiment', inner='stick', palette='Dark2')\n",
    "    sns.despine(top=True, right=True, bottom=True, left=True)\n",
    "\n",
    "\n",
    "    df_resampled = df['Sentiment Score'].resample('30T').mean()\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(df_resampled.index, df_resampled, linestyle='-', marker='o', alpha=0.7)\n",
    "\n",
    "    plt.title(\"7. Sentiment Score Over Time (Smoothed - 30 Min Avg)\", fontsize=14)\n",
    "    plt.xlabel(\"Time of Day\", fontsize=12)\n",
    "    plt.ylabel(\"Sentiment Score\", fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.gca().spines[['top', 'right']].set_visible(False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    df['Published'] = pd.to_datetime(df['Published'])\n",
    "    df.set_index('Published', inplace=True)\n",
    "\n",
    "    # Resampling Sentiment Scores over 30-minute intervals\n",
    "    df_resampled = df['Sentiment Score'].resample('30T').mean()\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(df_resampled.index, df_resampled, linestyle='-', marker='o', alpha=0.7, color='blue')\n",
    "\n",
    "    plt.title(\"8. Sentiment Score Trend Over Time (30-Min Avg)\", fontsize=14)\n",
    "    plt.xlabel(\"Time\", fontsize=12)\n",
    "    plt.ylabel(\"Sentiment Score\", fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.axhline(0, color='gray', linestyle='--')  # Neutral line\n",
    "    plt.gca().spines[['top', 'right']].set_visible(False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"No data available for visualization.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
